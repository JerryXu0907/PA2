# Implementing Backpropagation
In this assignment, we have been implementing a Multi-Layer Perceptron neural network using NumPy. We implemented the training and evaluation procedures to classify the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). 

## Dataset
Unzip ```data.zip``` to get the pickle files for train, validation and test splits of MNIST dataset. 

## Activation Functions
There are 3 activation functions (sigmoid, ReLU and tanh) to choose

## Layers
We calculate the gradient of the layers for back propogation. Matrix calculation was used to accelerate the training process

## Neural Network
Consists of layers and activation functions. This forms a true neural network that can both do well in forward and back prop.

## Problem Solving
All problem solving stuff has been written in a function, since we are not allowed to modify the main function. 
